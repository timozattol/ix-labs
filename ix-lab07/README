1
Spend some time inspecting the raw data. Are you able to find your favorite movie in? What
languages are the plots written in?
=> Favorite movie found, plots written in English.

2.2
 For starters, try to search for “James Bond 007”. What
do you think of the results? Try out different queries
=> Works very well, we also tried Lord of the rings, almost all the results are related to the search.

What movies do you get back when you use this description as a
query string? Can you explain why “Babe (1981)” has such a high ranking?
=> 
01. Paintbrush (2008) (score: 1.12490)
02. Babe (1981) (score: 0.97120)
03. Have I Shared Too Much? (2011) (score: 0.96333)
04. Eu Odeio o Orkut (2011) (score: 0.82655)
05. I Love Alaska (2009) (score: 0.81054)

The only common word is "modeling", this is strange. After searching for "modeling",
Babe is third. Maybe it's one of the only film that contains this word in its 
description, and thus "modeling" has a big tf-idf, boosting the cosine-similarity 
between the ix description and Babe.

3.1
We already coded this for you, simply run TermDocumentMatrix
on the cluster. How many rows and columns does the matrix have?
=> 196848 (n of movies) = columns
    ? (n of different terms) = rows
The matrix is binary, we don't know how to read from it 

Run the Concepts command on the cluster to get a feel of the concepts behind the first
few dimensions uncovered by the transformation. The first dimension is particularly striking.
How can you explain what you see?
=> Yes, the first dimension seems to classify terms as "common" or not. Words that appear in many descriptions have high value, words that appear in one of few descriptions have low value.
Dimension 2 seems to separates "art" <--> "violence"
Others are not very obvious

LSISearch. Run some queries and compare the results
to those obtained using the vector space model. What do you think of the results? Are they
better, worse? Can you get an intuition about what’s happening?
=> They are worse when we search for few words, and ok ish for a full description of a movie.
This is logical because searching for literally james bond will only get other james bond movies, 
whereas searching for the plot of james bond will give other action/thrillers when looking at concepts.

James Bond 007:
01. True Bond (2007) (TV) (score: 0.34947)
02. The Last Call (2012/I) (score: 0.33277)
03. La faute des autres (1923) (score: 0.33029)
04. Donna James & Gary (2006) (score: 0.32230)
05. A Boost of Love (2012) (score: 0.31848)
06. One Goal, One Hope (2010) (score: 0.31822)
07. Massa'ot James Be'eretz Hakodesh (2003) (score: 0.30906)
08. Down the Coast (2011) (score: 0.30686)
09. Above Suspicion (2000) (score: 0.30353)
10. On Her Majesty's Secret Service (1969) (score: 0.29841)
11. Resonance (2011) (score: 0.29829)
12. James Bond: Shaken and Stirred (1997) (TV) (score: 0.29609)
13. Casino Royale (1967) (score: 0.29152)
14. James Bond: Licence to Thrill (1987) (TV) (score: 0.28864)
15. Beyond Loch Ness (2008) (TV) (score: 0.28769)
16. Crystal Jam (2010) (score: 0.28709)
17. On/Off (2003) (score: 0.28609)
18. Protege (2009) (score: 0.28378)
19. James (2008) (score: 0.28241)
20. Someday This Pain Will Be Useful to You (2011) (score: 0.27679)



4
In the folder data/review-sample you will find a small sample of 8 reviews. Can you
tell5 which ones are deceptive, and which ones are trutfhul? Do you identify features that
help you telling them apart?
=> "Reference to other hotels" make us think that the review is truthful. Also when very precise "stories" are
told, it seems truthful. On the other hand, when the person employs generic words as "much, very, many, etc." or
is being vague, the review seems deceptive. But honestly it's hard to tell.

4.2
Add Laplace smoothing to your classifier, and check the performance for varying prior count values
(try 0.1, 1.0, 10.0) What accuracy can you reach?
=>

4.3
Can you write such a review? Use the class Filter to test your attempts. Suppose that you
are given access to the parameters of the classifier, can you come up with a principled approach to
craft a review that will be classified as truthful?
=>
