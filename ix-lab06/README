1.2
Run k-means on the synthetic dataset with k = 2 clusters. How does the distortion (fit of
data to the clusters) measure evolve? Analyze the clusters and assignments found.

=> The distortion converges to about 5000 after about 6 iterations.

Keeping the same number of clusters k, run your code on the dataset of tweets location.
Analyze the results you obtain by plotting the evolution of different clusters and their centers.
After convergence of k-means, how is the geographical distribution of the clusters? What
does the clustering obtained capture?

=> The tweets are divided in north-east and south-west. It does not capture manhattan very well, it even divides it in two.

1.3
What is the measure used in GMM that is the equivalent of the distortion measure in
k-means?
=> loglikelihood

• Run GMM with k = 2 components on the synthetic dataset. Interpret your results by
observing the means, covariance matrices, responsibilities and mixture coefficients. Compare
them to the parameters of the generating distribution.
=> They are very very similar

• Run GMM on the tweets dataset and vary the number of components k = 2, 3, 4. Interpret
the results you obtain by plotting the data points and the responsibilities of each cluster in
generating them.
=> 2 doesn't perform very well. 3 clusters gives us one in the middle of manhattan, one south and one north. 4 seems superfluous.

How sensitive is GMM to initial conditions? Explain your answer by pointing out potential
sources of instability.
=> Placing all centers in the same spot initially will create problems.

Give an example of a dataset where GMM might diverge.
Hint: Imagine a very popular tweeting-place in Manhattan.
=> 



2.4
Use script wiki_comm.py to print nb articles which are in the same community as the
article ArticleName:
=>